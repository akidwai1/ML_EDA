# -*- coding: utf-8 -*-
"""Copy of HW1_ML_try2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uSTqMHG0r3Vr2TfqF8nrCDPGAmj8wITi

Homework 1

exploratory data analysis (EDA):

1. Load data, prepare for analysis, process (if necessary)
"""

import pandas as pd

# Load the data
data = pd.read_csv('hw1_dataset.csv', sep='\t')  # Assuming tab-separated values based on the snippet

# Display the first few rows of the dataset
print(data.head())

# Check the data types of each feature
print(data.dtypes)

# Describe the dataset for preliminary analysis
print(data.describe())

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load the data
file_path = "hw1_dataset.csv"
data = pd.read_csv(file_path)
# Display the first few rows of the dataset
print(data.head())

import pandas as pd

# Load the dataset
file_path = 'hw1_dataset.csv'
data = pd.read_csv(file_path)

# View basic information about the dataset
print("Dataset Information:")
print(data.info())

# Check for null values
print("\nMissing Values in Each Column:")
print(data.isnull().sum())

# Display the first few rows of the dataset
print("\nFirst Few Rows of the Dataset:")
print(data.head())

"""2. Analyze Types of Data"""

# Analyze the types of data in each column
data_types = data.dtypes
print("Data Types:\n", data_types)

# Identify categorical and numerical columns
categorical_cols = data.select_dtypes(include=['object']).columns.tolist()
numerical_cols = data.select_dtypes(exclude=['object']).columns.tolist()

print("\nCategorical Columns:", categorical_cols)
print("Numerical Columns:", numerical_cols)

# Analyze data types
data_types = data.dtypes
print("\nData Types of Each Column:")
print(data_types)

# Separate numerical and categorical columns
numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = data.select_dtypes(include=['object']).columns

print("\nNumerical Columns:")
print(numerical_cols)

print("\nCategorical Columns:")
print(categorical_cols)

# Provide summary statistics for numerical features
print("\nSummary Statistics for Numerical Features:")
print(data[numerical_cols].describe())

"""3. Finding and Processing Missing and Erroneous Features"""

# Check for missing values
import pandas as pd
import numpy as np

# Load the dataset
data = pd.read_csv('hw1_dataset.csv')

#Step 1: Handling Missing Values
# Check for missing values in the dataset
missing_values = data.isnull().sum()
print("Missing values in each feature:\n", missing_values)

# Function to convert columns to numeric, coercing errors
def convert_to_numeric(column):
    return pd.to_numeric(column, errors='coerce')

# Convert relevant columns to numeric
data = data.apply(convert_to_numeric)

# Check again for non-numeric values and fill missing values
for column in data.columns:
    if data[column].isnull().any():  # Check if the column has missing values
        # Compute correlation of the current column with other columns
        correlations = data.corr()[column]

        # features that are positively correlated with the target column
        correlated_features = correlations[correlations > 0.5].index.tolist()  # Correlation threshold can be adjusted

        # Fill missing values with the median of correlated features
        median_value = data[correlated_features].median().median()  # Use median of the median
        data[column].fillna(median_value, inplace=True)

# After processing, check again for missing values to ensure they are filled
missing_values_after = data.isnull().sum()
print("Missing values after processing:\n", missing_values_after)

# Save cleaned data to a new CSV file
data.to_csv('hw1_dataset_cleaned.csv', index=False)

# Step 2: Identifying Erroneous Features
# Checking for numerical inconsistencies (e.g., negative values where they shouldn't be)
for col in data.select_dtypes(include=['float64', 'int64']).columns:
    if (data[col] < 0).any():
        print(f"\nColumn '{col}' contains negative values which may be erroneous.")

"""4. Find Outliers (if any)"""

import pandas as pd
import numpy as np

# Load the dataset
data = pd.read_csv('hw1_dataset.csv')

# Convert relevant columns to numeric and handle non-numeric values
data = data.apply(lambda col: pd.to_numeric(col, errors='coerce'))

# Define a function to identify outliers using the IQR method
def find_outliers_iqr(df):
    outliers = []

    for column in df.select_dtypes(include=[np.number]).columns:  # Only process numeric columns
        Q1 = df[column].quantile(0.25)  # First Quartile
        Q3 = df[column].quantile(0.75)  # Third Quartile
        IQR = Q3 - Q1  # Interquartile Range

        # Define bounds for outliers
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Find outliers
        column_outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]
        outliers.extend(column_outliers.tolist())  # Add to outliers list

    return outliers

# Find outliers in the dataset
outliers_list = find_outliers_iqr(data)

# Print the summary of outliers found
if outliers_list:
    print(f"Total number of outliers found: {len(outliers_list)}")
    print(f"Some outliers: {outliers_list[:10]}")  # Show only first 10 outliers as an example
else:
    print("No outliers found.")

# visualizing outliers using box plots
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Adjust number of columns based on the number of numerical features
num_cols = 5  # Number of columns in the grid
num_features = len(numerical_cols)
num_rows = int(np.ceil(num_features / num_cols))

# Create boxplots for each numerical feature
plt.figure(figsize=(num_cols * 4, num_rows * 4))  # Make the plot larger based on the number of features
for i, col in enumerate(numerical_cols):
    plt.subplot(num_rows, num_cols, i + 1)
    sns.boxplot(data[col])
    plt.title(col)
    plt.xlabel('Value')
plt.tight_layout()
plt.show()

""" 5. Find Highly Correlated Variables (if any)"""

import pandas as pd
import numpy as np

# Load the dataset
data = pd.read_csv('hw1_dataset.csv')

# Convert relevant columns to numeric and handle non-numeric values
data = data.apply(lambda col: pd.to_numeric(col, errors='coerce'))

# Calculate the correlation matrix
correlation_matrix = data.corr()

# Find highly correlated pairs (correlation > 0.75 or < -0.75)
threshold = 0.75
highly_correlated_vars = []

# Iterate over the correlation matrix to find pairs
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            highly_correlated_vars.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))

# Print summary of highly correlated variables found
if highly_correlated_vars:
    print("Highly correlated variables (correlation > 0.75 or < -0.75):")
    for var in highly_correlated_vars:
        print(f"{var[0]} and {var[1]}: Correlation = {var[2]:.2f}")
else:
    print("No highly correlated variables found.")

## IMPORTANT NOTE : Here , the heatmap has 1s in the middle because that typically reflects the correlation of each variable with itself, which is always 1
## so when both x and y axis are same the value comes out to be 1
## but it's not 1 between other pairs of variables!!

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('hw1_dataset.csv')

# Display the first few rows of the dataset
print("First few rows of the dataset:")
print(data.head())

# Check for non-numeric data
print("\nData types in the dataset:")
print(data.dtypes)

# Convert categorical columns to numeric (if any)
data_encoded = pd.get_dummies(data, drop_first=True)

# Handle NaN values, if any
data_encoded.fillna(data_encoded.mean(), inplace=True)

# Calculate the correlation matrix
correlation_matrix = data_encoded.corr()

# Get the absolute values of the correlation matrix and drop the target column
correlation_matrix = correlation_matrix.drop('experimental_proprty', axis=1)
correlation_matrix = correlation_matrix.abs()

# Set a threshold; here we consider a correlation coefficient greater than 0.8
threshold = 0.7
high_correlation_var = correlation_matrix[correlation_matrix > threshold]

# Find the highly correlated feature pairs
highly_correlated_pairs = [(col1, col2, corr) for col1 in high_correlation_var.columns for col2, corr in high_correlation_var[col1].items() if col1 != col2 and corr > threshold]

# Sort the pairs by correlation
highly_correlated_pairs.sort(key=lambda x: x[2], reverse=True)

# Get the unique variables from the pairs
unique_variables = set()
for col1, col2, corr in highly_correlated_pairs:
    unique_variables.add(col1)
    unique_variables.add(col2)

# Limit to 10 highly correlated variables
top_highly_correlated_variables = list(unique_variables)[:10]

# Print the top highly correlated variables
print("Top 10 highly correlated variables:")
print(top_highly_correlated_variables)

# Create the correlation matrix for these top variables
top_corr_matrix = data_encoded[top_highly_correlated_variables].corr()

# Generate a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(top_corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True, cbar_kws={"shrink": .8})
plt.title('Heatmap of Top 10 Highly Correlated Variables')
plt.show()

"""6. Find if the Target Variable is Correlated with Any Features"""

# target variable name
target_variable = 'experimental_proprty'

# Ensure the target variable is in the dataset and is numerical
if target_variable in data.columns:
    #  target variable is converted to numeric if it's not already
    data[target_variable] = pd.to_numeric(data[target_variable], errors='coerce')

    # Calculate correlations only with numerical variables and drop non-numeric ones
    numerical_data = data.select_dtypes(include=[np.number])
    target_correlations = numerical_data.corr()[target_variable].sort_values(ascending=False)

    print("\nCorrelation of Target Variable with Features:\n", target_correlations)
else:
    print(f"Target variable '{target_variable}' not found in the dataset.")

# bar plot 1
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# target variable name
target_variable = 'experimental_proprty'

#  target variable is in the dataset and is numerical
if target_variable in data.columns:
    # Convert the target variable to numeric if it's not already
    data[target_variable] = pd.to_numeric(data[target_variable], errors='coerce')

    # Calculate correlations only with numerical variables
    numerical_data = data.select_dtypes(include=[np.number])
    target_correlations = numerical_data.corr()[target_variable].sort_values(ascending=False)

    print("\nCorrelation of Target Variable with Features:\n", target_correlations)

    # Create the bar plot
    plt.figure(figsize=(12, 6))  # Increased width may help

    # number of features displayed to the top 10
    top_n = 10
    target_correlations.head(top_n).plot(kind='bar', color='skyblue')

    plt.title('Top Correlations of Target Variable with Features')
    plt.xlabel('Features')
    plt.ylabel('Correlation Coefficient')
    plt.axhline(0, color='k', linestyle='--')  # Reference line
    plt.xticks(rotation=45, ha='right')  # Rotate and align x-ticks for better clarity
    plt.tight_layout()  # Adjust layout to ensure everything fits
    plt.show()

else:
    print(f"Target variable '{target_variable}' not found in the dataset.")

"""7. Use PCA to plot data in 2D and color code by the target property. Do you see any patterns?"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Load the dataset
data = pd.read_csv('hw1_dataset.csv')

# Display the first few rows of the dataset
print("First few rows of the dataset:")
print(data.head())

# Encode categorical variables and handle NaN values
data_encoded = pd.get_dummies(data, drop_first=True)
data_encoded.fillna(data_encoded.mean(), inplace=True)

# Separate features and target variable
X = data_encoded.drop('experimental_proprty', axis=1)
y = data_encoded['experimental_proprty']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(X_scaled)

# Create a DataFrame for PCA results
pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
pca_df['Target Property'] = y.values

# Plot the PCA results
plt.figure(figsize=(10, 8))
sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Target Property', palette='viridis', s=100)
plt.title('PCA of Dataset Colored by Target Property')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid()
plt.show()

"""In the PCA plot, several patterns can be observed:

1. **Cluster Formation**: Distinct clusters based on color coding indicate that similar values of the target variable are grouped in specific areas of the PCA space. This suggests that certain features may have strong relationships with the target property.

2. **Separation of Groups**: Groups of similar values in the target variable appear to be well-separated along the principal component axes (PC1 and PC2), indicating that the features effectively distinguish between different states of the target variable. This separation shows potential for using these features in predictive modeling.

3. **Overlap and Ambiguity**: There are regions where points of different colors overlap, which indicates areas of uncertainty or similarity in feature values that do not clearly differentiate between different target property states.

4. **Outliers**: Points far from the main clusters may signify outliers or unique cases with extreme feature values that could skew analysis or indicate exceptional chemical properties.

Overall, the PCA visualization suggests that while there are defined patterns, certain complexities and ambiguities exist that needs further exploration.

Bonus Qs:

1. Use any non-linear dimensionality reduction method. Plot data in 2D and color code by the target property. Compare the resulting plot with PCA.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Load the dataset
data = pd.read_csv('hw1_dataset.csv')

# Define the target variable and features
target_variable = 'experimental_proprty'  # Ensure the correct name is used

# Convert target variable to numeric, handling errors
data[target_variable] = pd.to_numeric(data[target_variable], errors='coerce')

# Handle NaN values in the target variable
if data[target_variable].isnull().any():
    print("Warning: NaN values found in target variable. Dropping these rows.")
    data = data.dropna(subset=[target_variable])  # Drop rows where target is NaN

# Separate features and target variable
features = data.drop(columns=[target_variable])  # All features except the target
target = data[target_variable]

# Handle NaN values in the features
imputer = SimpleImputer(strategy='mean')  # Impute missing values with the mean
features_imputed = imputer.fit_transform(features.select_dtypes(include=[np.number]))

# Scale the features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features_imputed)

# Apply t-SNE
tsne = TSNE(n_components=2, random_state=42)
tsne_result = tsne.fit_transform(scaled_features)

# Create a DataFrame for t-SNE results
tsne_df = pd.DataFrame(data=tsne_result, columns=['TSNE1', 'TSNE2'])
tsne_df[target_variable] = target.reset_index(drop=True)  # Reset index to align

# Plotting
plt.figure(figsize=(10, 6))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue=target_variable, palette='viridis', alpha=0.7)
plt.title('t-SNE Plot with Color-Coding by Target Property')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.legend(title=target_variable, bbox_to_anchor=(1, 1), loc='upper left')
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Observations and Comparison

1. **Cluster Resolution**:
   - **t-SNE**: Generally provides better separation of clusters than PCA by preserving local neighbor information. Example : tighter clusters that better reflect the structure of the data.
   - **PCA**: While PCA attempts to preserve global variance, it may not resolve data localities as effectively, leading to overlapped clusters.

2. **Dimensionality Impact**:
   - Both methods create a 2D representation, but t-SNE's approach often reflects the significance of local data points, particularly beneficial in high-dimensional chemical property datasets.

3. **Outlier Representation**:
   - Outliers more pronounced in t-SNE plots. It identifies them clearly by placing them away from other clusters, highlighting their uniqueness.
   - In PCA, outliers can be visually easier to overlook, as they can be masked by compact clusters.

4. **Non-Linearity**:
   - t-SNE captures non-linear relationships between data points, potentially revealing complex structures that PCA fails to depict, particularly when feature manifolds are inherently non-linear.

2. Surprise us! Uncover hidden patterns and find non-trivial relationships in the data.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import umap  # UMAP import

# Load the data
data = pd.read_csv('hw1_dataset.csv')

# Define the target variable and features
target_variable = 'experimental_proprty'

# Convert target variable to numeric, handling errors
data[target_variable] = pd.to_numeric(data[target_variable], errors='coerce')

# Separate features and target variable
features = data.drop(columns=[target_variable])  # All features except the target
target = data[target_variable]

# Impute missing values for features
imputer = SimpleImputer(strategy='mean')  # Use mean imputation
features_imputed = imputer.fit_transform(features.select_dtypes(include=[np.number]))

# Scale the features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features_imputed)

# Apply t-SNE
tsne = TSNE(n_components=2, random_state=42)
tsne_result = tsne.fit_transform(scaled_features)

# Create a DataFrame for t-SNE results
tsne_df = pd.DataFrame(data=tsne_result, columns=['TSNE1', 'TSNE2'])
tsne_df[target_variable] = target

# Apply UMAP
umap_model = umap.UMAP(n_components=2, random_state=42)
umap_result = umap_model.fit_transform(scaled_features)

# Create a DataFrame for UMAP results
umap_df = pd.DataFrame(data=umap_result, columns=['UMAP1', 'UMAP2'])
umap_df[target_variable] = target

# Plotting t-SNE
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue=target_variable, palette='viridis', alpha=0.7)
plt.title('t-SNE Plot with Color-Coding by Target Property')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.legend(title=target_variable, bbox_to_anchor=(1, 1), loc='upper left')
plt.grid(True)

# Plotting UMAP
plt.subplot(1, 2, 2)
sns.scatterplot(data=umap_df, x='UMAP1', y='UMAP2', hue=target_variable, palette='viridis', alpha=0.7)
plt.title('UMAP Plot with Color-Coding by Target Property')
plt.xlabel('UMAP Component 1')
plt.ylabel('UMAP Component 2')
plt.legend(title=target_variable, bbox_to_anchor=(1, 1), loc='upper left')
plt.grid(True)

plt.tight_layout()
plt.show()

"""**Dimensionality Reduction:**

t-SNE: The script applies t-SNE to visualize high-dimensional data in two dimensions, generating a scatter plot that color-codes data points based on the target property.

UMAP: Similarly, UMAP is applied to reduce the dimensionality of the scaled features, producing another scatter plot with the same color-coding.

**Visualization:**

Both t-SNE and UMAP results are plotted side by side for comparison. Each plot highlights how data points relate to the target variable, helping to identify clusters and patterns.
"""